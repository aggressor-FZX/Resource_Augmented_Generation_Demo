{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d21b96c0",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Unstructured: “Document Understanding” & Chunking\n",
    "\n",
    "\n",
    "### Explanation for Code Block 1\n",
    "In this first cell, we pull down a series of WSU PDFs, merge them into one “mega-PDF,” and set up access to our vector database. We use **`requests`** to reliably download each file over HTTPS, and **`pypdf`**’s `PdfReader`/`PdfWriter` to concatenate all pages into a single document. The `glob` and `os` modules handle file paths and directories (`os.makedirs` ensures our `pdfs/` folder exists). After writing out `all_transfer_docs.pdf`, we load configuration values (API keys) from a separate `pinecone.init` file via **`configparser`**—this keeps secrets out of source control. Finally, we import the Pinecone client and instantiate it with our API key so we can start building a semantic index.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eea5bcfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded Transfer-Center-Brochure.pdf\n",
      "Downloaded admission.pdf\n",
      "Downloaded washington-45-list-of-one-year-transfer-courses.pdf\n",
      "Downloaded Student-Guide.pdf\n",
      "Downloaded Transfer-Student-Course-Planner-2023-2024-updated-1.pdf\n",
      "Downloaded WSU-Articulation-Handbook.pdf\n",
      "Downloaded Video-Text-Description_How-to-Send-Your-Transcripts-to-WSU.pdf\n",
      "Merged 7 files into all_transfer_docs.pdf\n",
      "c:\\Users\\jeffd\\OneDrive\\Desktop\\ConnectAI\\RAG_phase2\\.pine_env\\Scripts\\python.exe\n",
      "7.2.0\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pypdf import PdfReader, PdfWriter\n",
    "import glob\n",
    "import os\n",
    "\n",
    "pdf_urls = [\n",
    "    \"https://wpcdn.web.wsu.edu/wp-daesa/uploads/sites/3116/2024/04/Transfer-Center-Brochure.pdf\",\n",
    "    \"https://registrar.wsu.edu/media/fzepa10r/admission.pdf\",\n",
    "    \"https://em.wsu.edu/media/751323/washington-45-list-of-one-year-transfer-courses.pdf\",\n",
    "    \"https://s3.wp.wsu.edu/uploads/sites/917/2017/11/Student-Guide.pdf\",\n",
    "    \"https://s3.wp.wsu.edu/uploads/sites/16/2024/02/Transfer-Student-Course-Planner-2023-2024-updated-1.pdf\",\n",
    "    \"https://s3.wp.wsu.edu/uploads/sites/917/2015/07/WSU-Articulation-Handbook.pdf\",\n",
    "    \"https://s3.wp.wsu.edu/uploads/sites/2920/2024/10/Video-Text-Description_How-to-Send-Your-Transcripts-to-WSU.pdf\",\n",
    "]\n",
    "\n",
    "os.makedirs(\"pdfs\", exist_ok=True)\n",
    "# download files\n",
    "downloaded_paths = []\n",
    "for url in pdf_urls:\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    dest_path = os.path.join(\"pdfs\", filename)\n",
    "    resp = requests.get(url)\n",
    "    resp.raise_for_status()\n",
    "    with open(dest_path, \"wb\") as f:\n",
    "        f.write(resp.content)\n",
    "    downloaded_paths.append(dest_path)\n",
    "    print(f\"Downloaded {filename}\")\n",
    "\n",
    "# 4) Merge into one mega‐PDF\n",
    "writer = PdfWriter()\n",
    "for path in downloaded_paths:\n",
    "    reader = PdfReader(path)\n",
    "    for page in reader.pages:\n",
    "        writer.add_page(page)\n",
    "\n",
    "all_pdf = \"all_transfer_docs.pdf\"\n",
    "with open(all_pdf, \"wb\") as f:\n",
    "    writer.write(f)\n",
    "\n",
    "print(f\"Merged {len(downloaded_paths)} files into {all_pdf}\")\n",
    "\n",
    "import os\n",
    "import configparser\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "import sys\n",
    "print(sys.executable)\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "import pinecone\n",
    "# Set up your keys here\n",
    "\n",
    "# Load config This file is just a text file with stuff I want to keep seperate\n",
    "# This reads it and parses it so i can get the values I want\n",
    "config = configparser.ConfigParser()\n",
    "config.read('pinecone.init')\n",
    "PINECONE_API_KEY = config['DEFAULT']['PINECONE_API_KEY']\n",
    "OPENAI_API_KEY = config['DEFAULT']['OPENAI_API_KEY']\n",
    "\n",
    "print(pinecone.__version__) # Should be something like 4.1.0 \n",
    "\n",
    "# Connect to Pinecone\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec2d8ad",
   "metadata": {},
   "source": [
    "### Explanation for Code Block 2\n",
    "\n",
    "Here we transform the raw PDF into clean, chunked text ready for embedding. We rely on **Unstructured** (`partition_pdf`) to break the PDF into logical “elements” (text, tables, images) with OCR fallback via Tesseract, which we select by setting `OCR_AGENT`. Then, **LangChain**’s `RecursiveCharacterTextSplitter` slices long passages into overlapping chunks (500 characters with 100-char overlap) to preserve context edges. A helper `remove_odd_chars` collapses whitespace and strips non-ASCII noise. We also define `filter_by_type` to drop metadata fields Pinecone won’t accept (e.g., complex objects). Finally, we loop through each element, split and merge small fragments into sensible chunks, and save the result to `clean_text_cache.json` for fast reloads later.  \n",
    "# PDF Parsing, Chunking, and Vector Indexing with Pinecone\n",
    "\n",
    "This notebook walks through parsing PDFs with `unstructured`, splitting the extracted text into context-preserving chunks using LangChain, and pushing those chunks into a Pinecone index that automatically embeds and stores vectors.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Parse PDF with `partition_pdf`\n",
    "\n",
    "### Uses YOLOX for layout detection\n",
    "### Enables table extraction\n",
    "### Tesseract OCR language\n",
    "### Extract tables as base64 image blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a243f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pinecone: 7.2.0\n",
      "langchian: 0.3.26\n",
      "unstructured: 0.18.1\n",
      "unstructured-inference 1.0.5\n",
      "Prepared 519 chunks.\n"
     ]
    }
   ],
   "source": [
    "# Extract structured elements from PDF\n",
    "from importlib.metadata import version, PackageNotFoundError\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "import json\n",
    "import re\n",
    "\n",
    "def get_pkg_version(pkg_name: str) -> str:\n",
    "    try:\n",
    "        return version(pkg_name)\n",
    "    except PackageNotFoundError:\n",
    "        return f\"{pkg_name} not installed\"\n",
    "print(\"pinecone:\", get_pkg_version(\"pinecone\"))\n",
    "print(\"langchian:\", get_pkg_version(\"langchain\"))\n",
    "print(\"unstructured:\", get_pkg_version(\"unstructured\"))\n",
    "print(\"unstructured-inference\", get_pkg_version(\"unstructured-inference\"))   # should output 0.7.36\n",
    "# 1: Parse PDF into elements unstructured handles this part, AKA Partition the PDF to get raw elements\n",
    "#Setting the file here:\n",
    "\n",
    "\n",
    "os.environ[\"OCR_AGENT\"] = \"unstructured.partition.utils.ocr_models.tesseract_ocr.OCRAgentTesseract\"\n",
    "\n",
    "\n",
    "elements = partition_pdf(\n",
    "    filename=all_pdf,\n",
    "    strategy=\"hi_res\",\n",
    "    hi_res_model_name=\"yolox\",  # Instead of detectron2_onnx\n",
    "    skip_infer_table_types=[],            # Enable table extraction\n",
    "    languages=[\"eng\"],                    # Specify languages for OCR\n",
    "    extract_image_block_types=[\"Table\"]   # Extract table images as base64\n",
    ")\n",
    "def remove_odd_chars(text):\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # collapse whitespace\n",
    "    text = re.sub(r\"[^\\x20-\\x7E]\", \"\", text)  # remove non-ASCII characters\n",
    "    return text.strip()\n",
    "\n",
    "max_chunk_size = 500\n",
    "overlap_size = 100\n",
    "\n",
    "# This determines how to chunk our elements, overlap makes sure that context is not lost at edges\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=max_chunk_size, chunk_overlap=overlap_size)\n",
    "\n",
    "# helper function removes types that Pinecone doesn't like\n",
    "def filter_by_type(metadata):\n",
    "    filtered_data = {}\n",
    "    for k, v in metadata.items():\n",
    "        if isinstance(v, (str, int, float, bool)):\n",
    "            filtered_data[k] = v\n",
    "        # List of str,int,float,bool are ok too\n",
    "        elif isinstance(v, list) and all(isinstance(x, str) for x in v):\n",
    "            filtered_data[k] = v\n",
    "    return filtered_data\n",
    "\n",
    "# We need to manage chunk sizes and store their meta data. As well as clean it so this is a big loop\n",
    "clean_text = [] \n",
    "for elem in elements:\n",
    "    # Skip empties\n",
    "    if elem is None:\n",
    "        continue\n",
    "    # if we have an \"text\" attribute \n",
    "    elif hasattr(elem, \"text\"):\n",
    "\n",
    "        merged_chunks = []\n",
    "        buff = \"\"\n",
    "        chunks = splitter.split_text( remove_odd_chars(elem.text) )\n",
    "\n",
    "        # inner loop handles small chunks so we don't have noise/bloat \n",
    "        for ch in chunks: \n",
    "            if len(ch.strip()) < 40:\n",
    "                buff += \" \" + ch.strip()\n",
    "            else: \n",
    "                # we'll combine the small chunks into bigger ones\n",
    "                full_chunk = (buff+\" \"+ ch).strip()\n",
    "                merged_chunks.append(full_chunk)\n",
    "                buff = \"\"\n",
    "        if buff.strip():\n",
    "            merged_chunks.append(buff.strip())\n",
    "        \n",
    "        # Now each merged chunk is assigned the same meta data \n",
    "        for i, chunk in enumerate(merged_chunks):\n",
    "\n",
    "            # Return empty dict if elem has not metadata\n",
    "            metadata = elem.metadata.to_dict() if hasattr(elem, \"metadata\") else {} \n",
    "\n",
    "            # Go thru and remove unsupported types\n",
    "            filtered_metadata = filter_by_type(metadata)\n",
    "\n",
    "            # create the text to stor in vector DB\n",
    "            clean_text.append( { \"chunk_text\": chunk, \"metadata\" : filtered_metadata } )\n",
    "\n",
    "            \n",
    "             \n",
    "# Step 4: Prepare chunks for Pinecone\n",
    "print(f\"Prepared {len(clean_text)} chunks.\")\n",
    "\n",
    "import json\n",
    "\n",
    "with open(\"clean_text_cache.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(clean_text, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55444ac4",
   "metadata": {},
   "source": [
    "### Explanation for Code Block 2\n",
    "\n",
    "Here we transform the raw PDF into clean, chunked text ready for embedding. We rely on **Unstructured** (`partition_pdf`) to break the PDF into logical “elements” (text, tables, images) with OCR fallback via Tesseract, which we select by setting `OCR_AGENT`. Then, **LangChain**’s `RecursiveCharacterTextSplitter` slices long passages into overlapping chunks (500 characters with 100-char overlap) to preserve context edges. A helper `remove_odd_chars` collapses whitespace and strips non-ASCII noise. We also define `filter_by_type` to drop metadata fields Pinecone won’t accept (e.g., complex objects). Finally, we loop through each element, split and merge small fragments into sensible chunks, and save the result to `clean_text_cache.json` for fast reloads later.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e744fb46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index already exists, skipping creation.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Connect to Pinecone\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "# Create serverless index using llama-text-embed-v2 (auto-embedding)\n",
    "index_name = \"rag2riches\"\n",
    "\n",
    "if not pc.has_index(index_name):\n",
    "    pc.create_index_for_model(\n",
    "        name=index_name,\n",
    "        cloud=\"aws\",\n",
    "        region=\"us-east-1\",\n",
    "        embed={\n",
    "            \"model\": \"llama-text-embed-v2\",\n",
    "            \"field_map\": {\"text\": \"chunk_text\"}\n",
    "        }\n",
    "    )\n",
    "    print(\"Integrated embedding index created.\")\n",
    "else:\n",
    "    print(\"Index already exists, skipping creation.\")\n",
    "\n",
    "index = pc.Index(index_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608f41bf",
   "metadata": {},
   "source": [
    "### Explanation for Code Block 3\n",
    "\n",
    "This cell ensures our Pinecone index exists and is configured for automatic embeddings. We define a unique index name (`rag2riches`) and check with `pc.has_index()`. If absent, we call `pc.create_index_for_model`, telling Pinecone to use the **`llama-text-embed-v2`** model—this choice gives us state-of-the-art embeddings without manual preprocessing. The `field_map` argument instructs Pinecone to pull text from our `chunk_text` field. By centralizing index creation here, we avoid re-uploading or re-configuring on every run, speeding up development cycles. Finally, we bind `index = pc.Index(index_name)` so subsequent calls (upsert, query) know where to read and write.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e68098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing record: {'id': '0', 'text': 'About Us', 'detection_class_prob': 0.7652540802955627, 'last_modified': 'June_2025', 'languages': ['eng'], 'page_number': 1}\n",
      "{\n",
      "  \"id\": \"0\",\n",
      "  \"text\": \"About Us\",\n",
      "  \"detection_class_prob\": 0.7652540802955627,\n",
      "  \"last_modified\": \"June_2025\",\n",
      "  \"languages\": [\n",
      "    \"eng\"\n",
      "  ],\n",
      "  \"page_number\": 1\n",
      "}\n",
      "{\n",
      "  \"id\": \"0\",\n",
      "  \"text\": \"About Us\",\n",
      "  \"detection_class_prob\": 0.7652540802955627,\n",
      "  \"last_modified\": \"June_2025\",\n",
      "  \"languages\": [\n",
      "    \"eng\"\n",
      "  ],\n",
      "  \"page_number\": 1\n",
      "}\n",
      "{\n",
      "  \"id\": \"0\",\n",
      "  \"text\": \"About Us\",\n",
      "  \"detection_class_prob\": 0.7652540802955627,\n",
      "  \"last_modified\": \"June_2025\",\n",
      "  \"languages\": [\n",
      "    \"eng\"\n",
      "  ],\n",
      "  \"page_number\": 1\n",
      "}\n",
      "{\n",
      "  \"id\": \"0\",\n",
      "  \"text\": \"About Us\",\n",
      "  \"detection_class_prob\": 0.7652540802955627,\n",
      "  \"last_modified\": \"June_2025\",\n",
      "  \"languages\": [\n",
      "    \"eng\"\n",
      "  ],\n",
      "  \"page_number\": 1\n",
      "}\n",
      "{\n",
      "  \"id\": \"0\",\n",
      "  \"text\": \"About Us\",\n",
      "  \"detection_class_prob\": 0.7652540802955627,\n",
      "  \"last_modified\": \"June_2025\",\n",
      "  \"languages\": [\n",
      "    \"eng\"\n",
      "  ],\n",
      "  \"page_number\": 1\n",
      "}\n",
      "{\n",
      "  \"id\": \"0\",\n",
      "  \"text\": \"About Us\",\n",
      "  \"detection_class_prob\": 0.7652540802955627,\n",
      "  \"last_modified\": \"June_2025\",\n",
      "  \"languages\": [\n",
      "    \"eng\"\n",
      "  ],\n",
      "  \"page_number\": 1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Load pre-chunked documents, validate and filter metadata,\n",
    "construct Pinecone upsert records, and push them in batches.\n",
    "'''\n",
    "\n",
    "# Standard library imports\n",
    "import json  # For reading and writing JSON data\n",
    "from datetime import datetime  # For parsing and formatting date strings\n",
    "\n",
    "# Load the cached clean text chunks from disk\n",
    "with open(\"clean_text_cache.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    clean_text = json.load(f)\n",
    "\n",
    "def validate_metadata(metadata):\n",
    "    \"\"\"\n",
    "    Ensure all metadata fields are of an allowed type (str, int, float, bool, or list of str).\n",
    "    Print out any invalid fields for debugging.\n",
    "    \"\"\"\n",
    "    for key, value in metadata.items():\n",
    "        if isinstance(value, (str, int, float, bool)):\n",
    "            continue  # Basic scalar types are fine\n",
    "        elif isinstance(value, list) and all(isinstance(item, str) for item in value):\n",
    "            continue  # Lists of strings are allowed\n",
    "        else:\n",
    "            print(f\"Invalid field: {key} — type: {type(value)} — value: {value}\")\n",
    "\n",
    "# Prepare a list to accumulate Pinecone records\n",
    "records = []\n",
    "\n",
    "# Define keywords to exclude or remap in metadata\n",
    "excluded_keywords = [\"parent_id\"]\n",
    "\n",
    "# Iterate through each document chunk\n",
    "for idx, doc in enumerate(clean_text):\n",
    "    filtered_metadata = {}\n",
    "    metadata = doc.get(\"metadata\", {})\n",
    "\n",
    "    # Process each metadata key-value pair\n",
    "    for key, value in metadata.items():\n",
    "        if key == \"parent_id\":\n",
    "            # Remap 'parent_id' to a new metadata field 'source_doc'\n",
    "            filtered_metadata[\"source_doc\"] = value\n",
    "        elif any(term in key.lower() for term in excluded_keywords):\n",
    "            # Skip any other keys containing excluded keywords\n",
    "            continue\n",
    "        elif isinstance(value, str):\n",
    "            # Attempt to parse ISO date strings and reformat them\n",
    "            try:\n",
    "                dt = datetime.fromisoformat(value)\n",
    "                filtered_metadata[key] = dt.strftime(\"%B_%Y\")  # e.g., \"June_2025\"\n",
    "                continue  # Move to the next metadata item\n",
    "            except ValueError:\n",
    "                pass  # Not a date string; leave it for later checks\n",
    "        if isinstance(value, (str, int, float, bool)):\n",
    "            # Acceptable scalar metadata\n",
    "            filtered_metadata[key] = value\n",
    "        elif isinstance(value, list) and all(isinstance(item, str) for item in value):\n",
    "            # Acceptable list metadata\n",
    "            filtered_metadata[key] = value\n",
    "        else:\n",
    "            # Report and skip unexpected types\n",
    "            print(f\"Filtered out key: {key}, value: {value} (type: {type(value)})\")\n",
    "\n",
    "    # Validate the filtered metadata to catch any remaining issues\n",
    "    validate_metadata(filtered_metadata)\n",
    "\n",
    "    # Build the record to upsert into Pinecone\n",
    "    record = {\n",
    "        \"id\": str(idx),  # Unique string ID per chunk\n",
    "        \"text\": doc[\"chunk_text\"],\n",
    "        **filtered_metadata # * THIS UNPACKS THE DICTIONARY INTO TEXT! have to do this, can't give it dictionary or it breaks pinecone\n",
    "    }\n",
    "\n",
    "    # Print the first record for inspection\n",
    "    if idx == 0:\n",
    "        print(f\"Printing record: {record}\")\n",
    "\n",
    "    # Add to the list of records to upsert\n",
    "    records.append(record)\n",
    "\n",
    "def batch(iterable, batch_size):\n",
    "    \"\"\"\n",
    "    Yield successive batches of the given size from the iterable.\n",
    "    Pinecone limits batch upsert to 96 records at a time.\n",
    "    \"\"\"\n",
    "    for start in range(0, len(iterable), batch_size):\n",
    "        yield iterable[start : start + batch_size]\n",
    "\n",
    "# Upsert records into Pinecone in batches\n",
    "for batch_records in batch(records, 96):\n",
    "    # Print a sample record for logging (can remove in production)\n",
    "    print(json.dumps(records[0], indent=2))\n",
    "    index.upsert_records(namespace=\"default\", records=batch_records)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e7989a",
   "metadata": {},
   "source": [
    "### Explanation for Code Block 4\n",
    "\n",
    "Metadata must not be nested—flattening is required for Pinecone's schema.\n",
    "record = { ...}\n",
    "\n",
    "Each record is structured as a dict with id, text, and metadata fields.\n",
    "\n",
    "5. Recap: Workflow Stages\n",
    "Extract structured content from PDFs using unstructured.\n",
    "\n",
    "Slice content into manageable, overlapping chunks with LangChain.\n",
    "\n",
    "Assign metadata and unique IDs to each chunk.\n",
    "\n",
    "Use Pinecone’s auto-indexing to embed and store chunks for RAG or search.\n",
    "Now that our index is ready, we load the pre-chunked data and turn it into Pinecone “records.” We read `clean_text_cache.json` and validate each chunk’s metadata via `validate_metadata`, which prints any unsupported types. We then map each document into a dictionary with a unique `id`, the actual `text`, and filtered metadata (e.g., page number, source URL). Because Pinecone limits batch sizes to 96 items, we define a simple `batch()` generator that yields sublists of up to 96 records. Inside the loop, we print out the first record for sanity-checking, then call `index.upsert_records(namespace=\"default\", records=batch_records)` to bulk-load our text vectors into the cloud index.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "edf54d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "def query_with_rag(question, top_k=5):\n",
    "    # Semantic query\n",
    "    result = index.query(\n",
    "        top_k=top_k,\n",
    "        include_metadata=True,\n",
    "        vector=None,\n",
    "        query=question\n",
    "    )\n",
    "\n",
    "    # Build context from top results\n",
    "    context = \"\\n\\n\".join(\n",
    "        match[\"metadata\"].get(\"chunk_text\", \"\") for match in result[\"matches\"]\n",
    "    )\n",
    "\n",
    "    # Prompt OpenAI using the new client-based API\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant answering based only on the context provided.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Context:\\n{context}\\n\\nQuestion: {question}\"\n",
    "            }\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f82381",
   "metadata": {},
   "source": [
    "### Explanation for Code Block 5\n",
    "\n",
    "This cell defines our RAG (Retrieval-Augmented Generation) query function. We use the **OpenAI Python client** (`OpenAI(api_key=…)`) for a modern, unified API experience. Inside `query_with_rag`, we first call `index.query` to perform a semantic search over our embedded chunks (`top_k` controls how many hits). We then stitch together the top matches’ text into a single `context` string. Finally, we prompt GPT-4 with a system message locking it to “only the provided context,” followed by our user question. The temperature is set to 0 for deterministic, factual answers. The function returns the model’s reply, neatly wrapping search + generation in one call.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8613b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      " The context provided does not include specific information about the math requirements for entering as a junior in software engineering at WSU (Washington State University or any other institution referred to as WSU). Please refer to WSU's specific course requirements or contact the university directly for accurate information.\n"
     ]
    }
   ],
   "source": [
    "answer = query_with_rag(\"How advanced does my math need to be to enter as a junior in software engineering into wsu?\")\n",
    "print(\"Answer:\\n\", answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a83eafe",
   "metadata": {},
   "source": [
    "### Explanation for Code Block 6\n",
    "\n",
    "Here we demonstrate our end-to-end pipeline in action. By calling  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".pine_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
